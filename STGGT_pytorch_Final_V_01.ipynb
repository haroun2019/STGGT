{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_pP8YEELOemo"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7_0ZpQsLMc7n"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZMAJjlcTZha5"
      },
      "outputs": [],
      "source": [
        "#------------------------------------------------------------------------\n",
        "#-------------------------Imports----------------------------------------\n",
        "#------------------------------------------------------------------------\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import scipy.sparse as sp\n",
        "import torch\n",
        "from torch import nn\n",
        "import pickle\n",
        "\n",
        "from pandas import DataFrame\n",
        "import numpy.linalg as la\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy import stats\n",
        "import os\n",
        "import time\n",
        "import random\n",
        "import seaborn as sns\n",
        "import time\n",
        "import datetime\n",
        "import math\n",
        "import csv\n",
        "import networkx as nx\n",
        "\n",
        "from torch.nn.parameter import Parameter\n",
        "from torch.nn.modules.module import Module\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from torch.autograd import Variable\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8KMLPN69Of0J"
      },
      "source": [
        "# Variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yBD_uxUSOmTf"
      },
      "outputs": [],
      "source": [
        "#------------------------------------------------------------------------\n",
        "#-------------------------Variables--------------------------------------\n",
        "#------------------------------------------------------------------------\n",
        "data_name = 'la'  # 'la'--> METR-LA, 'bay'-->PEMS-BAY\n",
        "model_name =  'stat_pytorch_15'  #'stat_pytorch_15' 'transformer_pytorch_11'\n",
        "train_rate, val_rate = 0.7, 0.1  # 70%(train data), 10% (validation data) and 20%(test data)\n",
        "interval = 200 # for displaying results\n",
        "seq_len = 12  # Inputs sequence length\n",
        "num_runs = 1  # Test number\n",
        "\n",
        "pre_len = 3  ## Outputs sequence length\n",
        "\n",
        "if (data_name == 'la'):\n",
        "    adj_path =  '/content/drive/My Drive/Congestion/data/adj_mx.pkl' # path of adjacency matrix\n",
        "    start_date = '2012-03-01 00:00:00'  # Start of training data\n",
        "    test_date = '2012-06-27 23:55:00'   # Start of testing data\n",
        "    freq = '5min'  # Frequence\n",
        "\n",
        "    d_model = 207       # Model dimension 207\n",
        "    n_heads = 3         # Head number 3\n",
        "    FFN_units = 64      # Unit number 64\n",
        "    hid = [128, 64, 16] # Unit number of hidden layer [128, 64, 16]\n",
        "    n_layers_att = 1    # Layer number of attention 1\n",
        "    n_layers = 2        # Layer number 2\n",
        "    dropout = 0.6      # dropout 0.25\n",
        "\n",
        "    lr = 0.00125        # Learning rate 0.00125\n",
        "    decay_rate = 0.925  # decay rate 0.925\n",
        "    n_epochs = 100      # Epochs number 100\n",
        "    patience = 10       # patience 15\n",
        "    batch_size = 16     # batch size 16\n",
        "\n",
        "elif (data_name == 'bay'):\n",
        "    adj_path =  '/content/drive/My Drive/Congestion/data/adj_mx_bay.pkl' # path of adjacency matrix\n",
        "    start_date = '2017-01-01 00:00:00'    # Start of training data\n",
        "    test_date = '2017-06-30 22:55:00'     # Start of testing data\n",
        "    freq = '5min'   # Frequence\n",
        "\n",
        "    d_model = 325         # Model dimension 325\n",
        "    n_heads = 5           # Head number 5\n",
        "    FFN_units = 64        # Unit number 64\n",
        "    hid = [512, 128, 64]  # Unit number of hidden layer [512, 128, 64]\n",
        "    n_layers_att = 1      # Layer number of attention 1\n",
        "    n_layers = 2          # Layer number 2\n",
        "    dropout = 0.25        # dropout 0.25\n",
        "\n",
        "    lr = 0.00125          # Learning rate 0.00125\n",
        "    decay_rate = 0.95     # decay rate 0.95\n",
        "    n_epochs = 100        # Epochs number 100\n",
        "    patience = 15         # patience 15\n",
        "    batch_size = 64       # batch size 64\n",
        "\n",
        "else: print(\"Error.....choose another dataset\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2uVI5WNIOgcr"
      },
      "source": [
        "# Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6PhaTygLOm5O"
      },
      "outputs": [],
      "source": [
        "#------------------------------------------------------------------------\n",
        "#-------------------------Functions--------------------------------------\n",
        "#------------------------------------------------------------------------\n",
        "\n",
        "#------------------------------------------------------------------------\n",
        "#------------------------------------------------------------------------\n",
        "#-----------------------------make_dir-----------------------------------\n",
        "#------------------------------------------------------------------------\n",
        "def mkdir(path):\n",
        "    folder = os.path.exists(path)\n",
        "    if not folder:\n",
        "        os.makedirs(path)\n",
        "        print(\"---  new folder  ---\", path)\n",
        "    else:\n",
        "        print(\"---  There is this folder!  ---\", path)\n",
        "\n",
        "\n",
        "#------------------------------------------------------------------------\n",
        "#------------------------------------------------------------------------\n",
        "#-------------------------get_data---------------------------------------\n",
        "#------------------------------------------------------------------------\n",
        "def get_data(seq_len, pre_len, data_name):\n",
        "    data = load_data(data_name)\n",
        "    time_len, d_model = data.shape\n",
        "    rng = pd.date_range(start_date, periods=time_len, freq=freq)\n",
        "    data.index = rng\n",
        "\n",
        "    #----------split data----------------------------\n",
        "    trainX, trainY, valX, valY, testX, testY = split_data(data, train_rate, val_rate, seq_len, pre_len)\n",
        "\n",
        "    return trainX, trainY, valX, valY, testX, testY\n",
        "\n",
        "\n",
        "#------------------------------------------------------------------------\n",
        "#-------------------------load data--------------------------------------\n",
        "#------------------------------------------------------------------------\n",
        "def load_data(data_name=data_name):\n",
        "    if (data_name == 'la'):\n",
        "        data_path = r'/content/drive/My Drive/Congestion/metr_la.csv'\n",
        "    elif (data_name == 'bay'):\n",
        "        data_path = r'/content/drive/My Drive/Congestion/pems_bay.csv'\n",
        "    else:\n",
        "        return print(\"Error.....choose another dataset\")\n",
        "\n",
        "    data = pd.read_csv(data_path)\n",
        "    data.drop(columns=data.columns[0], axis=1, inplace=True)\n",
        "\n",
        "    return data\n",
        "\n",
        "#------------------------------------------------------------------------\n",
        "#-------------------------split_data-------------------------------------\n",
        "#------------------------------------------------------------------------\n",
        "def split_data(data, train_rate, val_rate, seq_len, pre_len):\n",
        "\n",
        "    dataX, dataY = [], []\n",
        "    for i in range(len(data) - (seq_len + pre_len)):\n",
        "        a = data[i: i + seq_len + pre_len]\n",
        "        dataX.append(a[0 : seq_len])\n",
        "        dataY.append(a[seq_len : seq_len + pre_len])\n",
        "\n",
        "    dataX, dataY = np.array(dataX), np.array(dataY)\n",
        "    data_size = dataX.shape[0]\n",
        "    train_size = int(data_size * train_rate)\n",
        "    val_size = int(data_size * val_rate)\n",
        "\n",
        "    trainX, trainY = dataX[0:train_size], dataY[0:train_size]\n",
        "    valX, valY = dataX[train_size:train_size + val_size], dataY[train_size:train_size + val_size]\n",
        "    testX, testY = dataX[train_size + val_size:data_size], dataY[train_size + val_size:data_size]\n",
        "\n",
        "    return trainX, trainY, valX, valY, testX, testY\n",
        "\n",
        "\n",
        "#------------------------------------------------------------------------\n",
        "#------------------------------------------------------------------------\n",
        "#---------------------------load_dataset---------------------------------\n",
        "#------------------------------------------------------------------------\n",
        "def load_dataset(device, trainX, trainY, valX, valY, testX, testY, batch_size):\n",
        "    data = {}\n",
        "    scaler = StandardScaler(mean=trainX.mean(), std=trainX.std())\n",
        "    trainX = scaler.transform(trainX)\n",
        "    valX = scaler.transform(valX)\n",
        "    testX = scaler.transform(testX)\n",
        "\n",
        "    data['train_loader'] = Data_Loader(trainX, trainY, batch_size)\n",
        "    data['val_loader'] = Data_Loader(valX, valY, batch_size)\n",
        "    data['test_loader'] = Data_Loader(testX, testY, batch_size)\n",
        "    data['scaler'] = scaler\n",
        "    return data\n",
        "\n",
        "\n",
        "#------------------------------------------------------------------------\n",
        "#-------------------------StandardScaler---------------------------------\n",
        "#------------------------------------------------------------------------\n",
        "class StandardScaler():\n",
        "\n",
        "    def __init__(self, mean, std, fill_zeroes=True):\n",
        "        self.mean = mean\n",
        "        self.std = std\n",
        "        self.fill_zeroes = fill_zeroes\n",
        "\n",
        "    def transform(self, data):\n",
        "        if self.fill_zeroes:\n",
        "            mask = (data == 0)\n",
        "            data[mask] = self.mean\n",
        "        return (data - self.mean) / self.std\n",
        "\n",
        "    def inverse_transform(self, data):\n",
        "        return (data * self.std) + self.mean\n",
        "\n",
        "\n",
        "#------------------------------------------------------------------------\n",
        "#-------------------------Data_Loader------------------------------------\n",
        "#------------------------------------------------------------------------\n",
        "class Data_Loader(object):\n",
        "    def __init__(self, xs, ys, batch_size):\n",
        "\n",
        "        self.batch_size = batch_size\n",
        "        self.current_ind = 0\n",
        "        self.size = len(xs)\n",
        "        self.num_batch = int(self.size // self.batch_size)\n",
        "        self.xs = xs\n",
        "        self.ys = ys\n",
        "\n",
        "    def shuffle(self):\n",
        "        permutation = np.random.permutation(self.size)\n",
        "        xs, ys = self.xs[permutation], self.ys[permutation]\n",
        "        self.xs = xs\n",
        "        self.ys = ys\n",
        "\n",
        "    def get_iterator(self):\n",
        "        self.current_ind = 0\n",
        "\n",
        "        def _wrapper():\n",
        "            while self.current_ind < self.num_batch:\n",
        "                start_ind = self.batch_size * self.current_ind\n",
        "                end_ind = min(self.size, self.batch_size * (self.current_ind + 1))\n",
        "                x_i = self.xs[start_ind: end_ind, ...]\n",
        "                y_i = self.ys[start_ind: end_ind, ...]\n",
        "                yield (x_i, y_i)\n",
        "                self.current_ind += 1\n",
        "\n",
        "        return _wrapper()\n",
        "\n",
        "\n",
        "#------------------------------------------------------------------------\n",
        "#------------------------------------------------------------------------\n",
        "#-----------------load_pickle (Adjacency matrix)-------------------------\n",
        "#------------------------------------------------------------------------\n",
        "def load_pickle(pickle_file):\n",
        "    try:\n",
        "        with open(pickle_file, 'rb') as f:\n",
        "            pickle_data = pickle.load(f)\n",
        "    except UnicodeDecodeError as e:\n",
        "        with open(pickle_file, 'rb') as f:\n",
        "            pickle_data = pickle.load(f, encoding='latin1')\n",
        "    except Exception as e:\n",
        "        print('Unable to load data ', pickle_file, ':', e)\n",
        "        raise\n",
        "    return pickle_data\n",
        "\n",
        "\n",
        "#------------------------------------------------------------------------\n",
        "#--------------------Adjacency matrix normalization----------------------\n",
        "#------------------------------------------------------------------------\n",
        "def get_normalized_adj(A):\n",
        "\n",
        "    I = np.eye(A.shape[0], dtype=np.float32)\n",
        "    A_hat = A + I # add self-loops\n",
        "    D_hat_diag = np.sum(A_hat, axis=1)\n",
        "    D_hat_diag_inv_sqrt = np.power(D_hat_diag, -0.5)\n",
        "    D_hat_diag_inv_sqrt[np.isinf(D_hat_diag_inv_sqrt)] = 0.\n",
        "    D_hat_inv_sqrt = np.diag(D_hat_diag_inv_sqrt)\n",
        "    return np.dot(np.dot(D_hat_inv_sqrt, A_hat), D_hat_inv_sqrt)\n",
        "\n",
        "\n",
        "#------------------------------------------------------------------------\n",
        "#------------------------------------------------------------------------\n",
        "#-------------------------learning rate----------------------------------\n",
        "#------------------------------------------------------------------------\n",
        "def learning_r(epoch, decay_rate):\n",
        "    return decay_rate**epoch\n",
        "\n",
        "\n",
        "#------------------------------------------------------------------------\n",
        "#------------------------------------------------------------------------\n",
        "#-------------------------evaluation-------------------------------------\n",
        "#------------------------------------------------------------------------\n",
        "def evaluate(pred, target):\n",
        "    mape = loss_mape(pred, target, 0.0).item()\n",
        "    rmse = loss_rmse(pred, target, 0.0).item()\n",
        "    mae = loss_mae(pred, target, 0.0).item()\n",
        "    return mape, rmse, mae\n",
        "\n",
        "\n",
        "#------------------------------------------------------------------------\n",
        "#-------------------------loss_mae---------------------------------------\n",
        "#------------------------------------------------------------------------\n",
        "def loss_mae(preds, labels, null_val=np.nan):\n",
        "#    print('preds', preds.shape)\n",
        "#    print('labels', labels.shape)\n",
        "    if np.isnan(null_val):\n",
        "        mask = ~torch.isnan(labels)\n",
        "    else:\n",
        "        mask = (labels != null_val)\n",
        "    mask = mask.float()\n",
        "    mask /= torch.mean((mask))\n",
        "    mask = torch.where(torch.isnan(mask), torch.zeros_like(mask), mask)\n",
        "    loss = torch.abs(preds - labels)\n",
        "    loss = loss * mask\n",
        "    loss = torch.where(torch.isnan(loss), torch.zeros_like(loss), loss)\n",
        "    return torch.mean(loss)\n",
        "\n",
        "\n",
        "#------------------------------------------------------------------------\n",
        "#-------------------------loss_rmse--------------------------------------\n",
        "#------------------------------------------------------------------------\n",
        "def loss_rmse(preds, labels, null_val=np.nan):\n",
        "    if np.isnan(null_val):\n",
        "        mask = ~torch.isnan(labels)\n",
        "    else:\n",
        "        mask = (labels != null_val)\n",
        "    mask = mask.float()\n",
        "    mask /= torch.mean((mask))\n",
        "    mask = torch.where(torch.isnan(mask), torch.zeros_like(mask), mask)\n",
        "    loss = (preds - labels)** 2\n",
        "    loss = loss * mask\n",
        "    loss = torch.where(torch.isnan(loss), torch.zeros_like(loss), loss)\n",
        "    return torch.sqrt(torch.mean(loss))\n",
        "\n",
        "\n",
        "#------------------------------------------------------------------------\n",
        "#-------------------------loss_mape--------------------------------------\n",
        "#------------------------------------------------------------------------\n",
        "def loss_mape(preds, labels, null_val=np.nan):\n",
        "    if np.isnan(null_val):\n",
        "        mask = ~torch.isnan(labels)\n",
        "    else:\n",
        "        mask = (labels != null_val)\n",
        "    # print(\"mask:\",type(mask))\n",
        "    mask = mask.float()\n",
        "    mask /= torch.mean((mask))\n",
        "    mask = torch.where(torch.isnan(mask), torch.zeros_like(mask), mask)\n",
        "    loss = torch.abs((preds - labels) / labels)\n",
        "    loss = loss * mask\n",
        "    loss = torch.where(torch.isnan(loss), torch.zeros_like(loss), loss)\n",
        "    return torch.mean(loss)*100\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iX3Uc58Tlexi"
      },
      "source": [
        "# Positional Encoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YxVQ845Rm-m-"
      },
      "outputs": [],
      "source": [
        "#------------------------------------------------------------------------\n",
        "#-------------------------PositionalEncoding-----------------------------\n",
        "#------------------------------------------------------------------------\n",
        "class PositionalEncoding(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model, max_len, device):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "\n",
        "    def get_angles(self, pos, i, d_model): # pos: (seq_length, 1) i: (1, d_model)\n",
        "        angles = 1 / np.power(10000., (2*(i//2)) / np.float32(d_model))\n",
        "        return pos * angles # (seq_length, d_model)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        # input shape batch_size, seq_length, d_model\n",
        "        seq_length = inputs.size(-2)\n",
        "        d_model = inputs.size(-1)\n",
        "        # Calculate the angles given the input\n",
        "        angles = self.get_angles(np.arange(seq_length)[:, np.newaxis],\n",
        "                                 np.arange(d_model)[np.newaxis, :],\n",
        "                                 d_model)\n",
        "        # Calculate the positional encodings\n",
        "        angles[:, 0::2] = np.sin(angles[:, 0::2])\n",
        "        angles[:, 1::2] = np.cos(angles[:, 1::2])\n",
        "        # Expand the encodings with a new dimension\n",
        "        pos_encoding = angles[np.newaxis, ...]\n",
        "        pos_encoding = torch.from_numpy(pos_encoding).to(torch.float32).to(device)\n",
        "#        pos_encoding = torch.from_numpy(pos_encoding).to(torch.float32)\n",
        "\n",
        "        return inputs + pos_encoding\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K-uAyJ8pF3gz"
      },
      "source": [
        "# GCN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w4fo60YHNZdA"
      },
      "outputs": [],
      "source": [
        "#------------------------------------------------------------------------\n",
        "#-------------------------------GCN--------------------------------------\n",
        "#------------------------------------------------------------------------\n",
        "class GCN(nn.Module):\n",
        "    def __init__(self, node_features, hidden_dim, num_classes, dropout, n_layers, use_bias=True):\n",
        "        super(GCN, self).__init__()\n",
        "        self.n_layers = n_layers\n",
        "        self.gcn_1 = GCNLayer(node_features, hidden_dim, use_bias)\n",
        "        self.gcn_2 = GCNLayer(hidden_dim, num_classes, use_bias)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def initialize_weights(self):\n",
        "        self.gcn.initialize_weights()\n",
        "        self.gcn_1.initialize_weights()\n",
        "        self.gcn_2.initialize_weights()\n",
        "\n",
        "    def forward(self, x, adj):\n",
        "        x = x.permute(1, 0, 2)\n",
        "        x = F.relu(self.gcn_1(x, adj))\n",
        "        x = self.dropout(x)\n",
        "        x = self.gcn_2(x, adj)\n",
        "        x = x.permute(1, 2, 0)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "#------------------------------------------------------------------------\n",
        "#-------------------------------GCNLayer---------------------------------\n",
        "#------------------------------------------------------------------------\n",
        "class GCNLayer(nn.Module):\n",
        "    def __init__(self, in_features, out_features, use_bias=True):\n",
        "        super(GCNLayer, self).__init__()\n",
        "        self.weight = nn.Parameter(torch.FloatTensor(torch.zeros(size=(in_features, out_features))))\n",
        "        if use_bias:\n",
        "            self.bias = nn.Parameter(torch.FloatTensor(torch.zeros(size=(out_features,))))\n",
        "        else:\n",
        "            self.register_parameter('bias', None)\n",
        "        self.in_features = in_features\n",
        "        self.out_features = out_features\n",
        "\n",
        "        self.initialize_weights()\n",
        "\n",
        "    def initialize_weights(self):\n",
        "        nn.init.xavier_uniform_(self.weight)\n",
        "        if self.bias is not None:\n",
        "            nn.init.zeros_(self.bias)\n",
        "\n",
        "    def forward(self, x, adj):\n",
        "        d_model, batch, seq = x.size()\n",
        "        x = torch.reshape(x, (d_model, -1))\n",
        "        x = torch.spmm(adj, x)\n",
        "        x = torch.reshape(x, (d_model, batch, -1))\n",
        "        x = torch.reshape(x, (d_model * batch, -1))\n",
        "        x = x @ self.weight\n",
        "        if self.bias is not None:\n",
        "            x += self.bias\n",
        "        x = torch.reshape(x, (-1, batch*self.out_features))\n",
        "        x = torch.reshape(x, (-1, batch, self.out_features))\n",
        "\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YbDkOzu6oUCI"
      },
      "source": [
        "# GRU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2amywM0CoXCX"
      },
      "outputs": [],
      "source": [
        "#------------------------------------------------------------------------\n",
        "#-------------------------------GRU--------------------------------------\n",
        "#------------------------------------------------------------------------\n",
        "class GRU(nn.Module):\n",
        "\n",
        "    def __init__(self, input_size, hidden_size, num_classes, num_layers):\n",
        "        super(GRU, self).__init__()\n",
        "\n",
        "        self.gru = nn.GRU(input_size=input_size,\n",
        "                            hidden_size=hidden_size,\n",
        "                            num_layers=num_layers,\n",
        "                            batch_first=True)\n",
        "\n",
        "        self.fc = nn.Linear(hidden_size, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        x, (h_n, h_c) = self.gru(x, None)\n",
        "        x = self.fc(x)\n",
        "\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tGPqxeIUOgza"
      },
      "source": [
        "# Attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ab5NG9wo1Bsd"
      },
      "outputs": [],
      "source": [
        "#------------------------------------------------------------------------\n",
        "#-------------------------------Attention--------------------------------\n",
        "#------------------------------------------------------------------------\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, D, H, FFN_units, dropout_rate):\n",
        "        super(Attention, self).__init__()\n",
        "\n",
        "        self.mha1 = MultiHeadAttention(D, H)\n",
        "        self.dropout1 = nn.Dropout(dropout_rate)\n",
        "        self.layernorm1 = nn.LayerNorm(D, eps=1e-12) # 1e-9/1e-12\n",
        "\n",
        "        self.mlp_hidden = nn.Linear(D, FFN_units)\n",
        "        self.mlp_out = nn.Linear(FFN_units, D)\n",
        "        self.dropout2 = nn.Dropout(dropout_rate)\n",
        "        self.layernorm2 = nn.LayerNorm(D, eps=1e-12) # 1e-9/1e-12\n",
        "\n",
        "    def forward(self, dec, enc, mask1):\n",
        "\n",
        "        attn1, attn_weights1 = self.mha1(q=dec, k=enc, v=enc, mask=mask1)  # (B, S, D)\n",
        "        attn1 = self.dropout2(attn1) # (B,S,D)\n",
        "        attn1 = self.layernorm2(attn1 + dec) # (B,S,D)\n",
        "\n",
        "        mlp_act = F.relu(self.mlp_hidden(attn1))\n",
        "        mlp_act = self.mlp_out(mlp_act)\n",
        "        mlp_act = self.dropout2(mlp_act)\n",
        "        output = self.layernorm2(mlp_act + attn1)  # (B, S, D)\n",
        "\n",
        "        return output, attn_weights1\n",
        "\n",
        "\n",
        "#------------------------------------------------------------------------\n",
        "#--------------------------MultiHeadAttention----------------------------\n",
        "#------------------------------------------------------------------------\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    '''Multi-head self-attention module'''\n",
        "    def __init__(self, D, H):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.H = H # number of heads\n",
        "        self.D = D # model dimension\n",
        "\n",
        "        self.wq = nn.Linear(D, D)\n",
        "        self.wk = nn.Linear(D, D)\n",
        "        self.wv = nn.Linear(D, D)\n",
        "\n",
        "        self.dense = nn.Linear(D, D)\n",
        "\n",
        "    def split_heads(self, tensor):\n",
        "\n",
        "        batch_size, length, d_model = tensor.size()\n",
        "\n",
        "        d_tensor = d_model // self.H\n",
        "        tensor = tensor.view(batch_size, length, self.H, d_tensor).transpose(1, 2)\n",
        "\n",
        "        return tensor\n",
        "\n",
        "    def concat_heads(self, tensor):\n",
        "\n",
        "        batch_size, head, length, d_tensor = tensor.size()\n",
        "        d_model = head * d_tensor\n",
        "\n",
        "        tensor = tensor.transpose(1, 2).contiguous().view(batch_size, length, d_model)\n",
        "        return tensor\n",
        "\n",
        "    def forward(self, q, k, v, mask):\n",
        "\n",
        "        q = self.wq(q)  # (B, S, d*H)\n",
        "        k = self.wk(k)  # (B, S, d*H)\n",
        "        v = self.wv(v)  # (B, S, d*H)\n",
        "\n",
        "        q = self.split_heads(q)  # (B, H, S, d)\n",
        "        k = self.split_heads(k)  # (B, H, S, d)\n",
        "        v = self.split_heads(v)  # (B, H, S, d)\n",
        "\n",
        "        attention_scores = torch.matmul(q, k.transpose(-1, -2)) #(B,H,S,S)\n",
        "        attention_scores = attention_scores / math.sqrt(self.D // self.H)\n",
        "\n",
        "        # add the mask to the scaled tensor.\n",
        "        if mask is not None:\n",
        "            attention_scores += (mask * -1e9)\n",
        "\n",
        "        attention_weights = nn.Softmax(dim=-1)(attention_scores)\n",
        "        scaled_attention = torch.matmul(attention_weights, v)  # (B, H, S, d)\n",
        "        concat_attention = self.concat_heads(scaled_attention) # (B, S, d*H)\n",
        "        output = self.dense(concat_attention)  # (B, S, D)\n",
        "        return output, attention_weights\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "amBbKHoqOiUW"
      },
      "source": [
        "# STAT_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hq5sNwWTvgxH"
      },
      "outputs": [],
      "source": [
        "#------------------------------------------------------------------------\n",
        "#-------------------------------STAT_model-------------------------------\n",
        "#------------------------------------------------------------------------\n",
        "class STAT_model(nn.Module):\n",
        "\n",
        "    def __init__(self, device, d_model, n_layers_att, n_layers, FFN_units, hid, dropout_rate):\n",
        "        super(STAT_model, self).__init__()\n",
        "\n",
        "        self.device = device\n",
        "        self.n_layers_att = n_layers_att\n",
        "\n",
        "        self.adj = self.get_adjacency(adj_path, device)\n",
        "        self.gcn = GCN(seq_len, FFN_units, seq_len, dropout_rate, n_layers)\n",
        "\n",
        "        self.pos_encoding = PositionalEncoding(d_model, seq_len, device)\n",
        "        self.attention = nn.ModuleList([Attention(d_model,\n",
        "                                                   n_heads,\n",
        "                                                   FFN_units,\n",
        "                                                   dropout_rate\n",
        "                                       ) for i in range(n_layers_att)])\n",
        "\n",
        "        self.gru = GRU(seq_len, hid[0], hid[1], n_layers)\n",
        "        self.output1 = nn.Linear(hid[1], hid[2])\n",
        "        self.output2 = nn.Linear(hid[2], pre_len)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        x_ = x.permute(0, 2, 1)\n",
        "        x = self.gcn(x, self.adj)\n",
        "\n",
        "        x_ = self.pos_encoding(x_)\n",
        "\n",
        "        mask1 = None\n",
        "        for i in range(self.n_layers_att):\n",
        "            x, block = self.attention[i](x_, x, mask1)\n",
        "\n",
        "        x = x.permute(0, 2, 1)\n",
        "\n",
        "        x = F.relu(self.gru(x))\n",
        "\n",
        "        x = self.output1(x)\n",
        "        x = self.output2(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def get_adjacency(self, adj_path, device):\n",
        "        _, _, A = load_pickle(adj_path)\n",
        "        adj = get_normalized_adj(A)\n",
        "        adj = torch.from_numpy(adj).to(device)\n",
        "        return adj\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RAHA_LRFOi78"
      },
      "source": [
        "# Main"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HyZRSgFxlgdx"
      },
      "outputs": [],
      "source": [
        "#------------------------------------------------------------------------\n",
        "#------------------------------Main--------------------------------------\n",
        "#------------------------------------------------------------------------\n",
        "train_continue = False  # True, False\n",
        "check_point_exist = False  # True, False\n",
        "\n",
        "for run in range(num_runs): # num_runs : Test number\n",
        "\n",
        "    record = []\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "        device = torch.device(\"cuda:0\")\n",
        "        print(\"Let's use {} GPU!\".format(device))\n",
        "    else:\n",
        "        device = torch.device(\"cpu\")\n",
        "\n",
        "    save = '/content/drive/My Drive/Congestion/JA_model_pytorch/%s/%s/epoch%r'%(model_name,data_name,n_epochs)\n",
        "    mkdir(save)\n",
        "    path_1 = 'seq%r_pre%r_run%r_'%(seq_len, pre_len, run+1)+'check_point_model.pt'\n",
        "    check_point_path = os.path.join(save,path_1)\n",
        "    record_path = f'{save}/seq%r_pre%r_run%r_'%(seq_len, pre_len, run+1)+'record.csv'\n",
        "\n",
        "    #----------------------------------------------------------------------\n",
        "    #---------------------------Preparing data-----------------------------\n",
        "    #----------------------------------------------------------------------\n",
        "    trainX, trainY, valX, valY, testX, testY = get_data(seq_len, pre_len, data_name)\n",
        "    d_model = trainX.shape[-1]\n",
        "    dataloader = load_dataset(device, trainX, trainY, valX, valY, testX, testY, batch_size)\n",
        "    scaler = dataloader['scaler']\n",
        "\n",
        "    #----------------------------------------------------------------------\n",
        "    #---------------------------Defining model-----------------------------\n",
        "    #----------------------------------------------------------------------\n",
        "    model = STAT_model(device = device,\n",
        "                        d_model = d_model,\n",
        "                        n_layers_att = n_layers_att,\n",
        "                        n_layers = n_layers,\n",
        "                        FFN_units = FFN_units,\n",
        "                        hid = hid,\n",
        "                        dropout_rate = dropout\n",
        "                        ).to(device)\n",
        "    model.to(device)\n",
        "    model.zero_grad()\n",
        "    # Loss and optimizer\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "    optimizer.zero_grad()\n",
        "    scheduler = torch.optim.lr_scheduler.LambdaLR(\n",
        "            optimizer, lr_lambda=lambda epoch: learning_r(epoch, decay_rate))\n",
        "    criterion = loss_mae\n",
        "\n",
        "    #----------------------------------------------------------------------\n",
        "    #----------------Loading model with the last parameters----------------\n",
        "    #----------------------------------------------------------------------\n",
        "    if train_continue:\n",
        "        print(\"reload the model from :{}\", check_point_path)\n",
        "        checkpoint = torch.load(check_point_path)\n",
        "        model.load_state_dict(checkpoint['model_state_dict'])\n",
        "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "\n",
        "\n",
        "    print('\\n====================Run N°: {}================='.format(run+1))\n",
        "    his_loss, val_time, train_time = [], [], []\n",
        "    trigger_times = 0\n",
        "\n",
        "    for epoch in range(n_epochs):\n",
        "        print('-' * 10)\n",
        "        print('Epoch {}/{}'.format(epoch+1, n_epochs))\n",
        "\n",
        "        #----------------------------------------------------------------------\n",
        "        #------------------------------Training--------------------------------\n",
        "        #----------------------------------------------------------------------\n",
        "        train_loss, train_mape, train_rmse, train_mae = [], [], [], []\n",
        "        t1, t = time.time(), time.time()\n",
        "        dataloader['train_loader'].shuffle()\n",
        "        for iter, (x, y) in enumerate(dataloader['train_loader'].get_iterator()):\n",
        "            trainX = torch.Tensor(x).to(device).transpose(1, 2)\n",
        "            trainY = torch.Tensor(y).to(device).transpose(1, 2)\n",
        "\n",
        "            model.train()\n",
        "            y_pred = model(trainX)\n",
        "            y_pred = scaler.inverse_transform(y_pred)\n",
        "            loss = criterion(y_pred, trainY, 0.0)\n",
        "\n",
        "            # Backward pass and update\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # zero grad before new step\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            evaluation = evaluate(y_pred, trainY)\n",
        "            train_loss.append(loss.item())\n",
        "            train_mape.append(evaluation[0])\n",
        "            train_rmse.append(evaluation[1])\n",
        "            train_mae.append(evaluation[2])\n",
        "\n",
        "            if iter % interval == 0:\n",
        "                log = 'Iter: {:04d}|Train Loss: {:.4f}|Train MAPE: {:.4f}|Train RMSE: {:.4f}|Train MAE: {:.4f}' \\\n",
        "                      '|Time: {:.4f} '\n",
        "                print(log.format(iter,train_loss[-1],train_mape[-1],train_rmse[-1],train_mae[-1],time.time()-t),\n",
        "                      flush=True)\n",
        "                t = time.time()\n",
        "        scheduler.step()\n",
        "        t2 = time.time()\n",
        "        train_time.append(t2 - t1)\n",
        "\n",
        "        #----------------------------------------------------------------------\n",
        "        #------------------------------Validation------------------------------\n",
        "        #----------------------------------------------------------------------\n",
        "        valid_loss, valid_mape, valid_rmse, valid_mae = [], [], [], []\n",
        "        s1 = time.time()\n",
        "        for iter, (x, y) in enumerate(dataloader['val_loader'].get_iterator()):\n",
        "            valX = torch.Tensor(x).to(device).transpose(1, 2)\n",
        "            valY = torch.Tensor(y).to(device).transpose(1, 2)\n",
        "\n",
        "            model.eval()\n",
        "            y_pred = model(valX)\n",
        "            y_pred = scaler.inverse_transform(y_pred)\n",
        "            loss = criterion(y_pred, valY, 0.0)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            evaluation = evaluate(y_pred, valY)\n",
        "            valid_loss.append(loss.item())\n",
        "            valid_mape.append(evaluation[0])\n",
        "            valid_rmse.append(evaluation[1])\n",
        "            valid_mae.append(evaluation[2])\n",
        "\n",
        "        s2 = time.time()\n",
        "        log = 'Epoch: {:03d}, Inference Time: {:.4f} secs'\n",
        "        print(log.format(epoch+1, (s2 - s1)))\n",
        "        val_time.append(s2 - s1)\n",
        "\n",
        "        mtrain_loss = np.mean(train_loss)\n",
        "        mtrain_mape = np.mean(train_mape)\n",
        "        mtrain_rmse = np.mean(train_rmse)\n",
        "        mtrain_mae = np.mean(train_mae)\n",
        "\n",
        "        mvalid_loss = np.mean(valid_loss)\n",
        "        mvalid_mape = np.mean(valid_mape)\n",
        "        mvalid_rmse = np.mean(valid_rmse)\n",
        "        mvalid_mae = np.mean(valid_mae)\n",
        "        his_loss.append(mvalid_loss)\n",
        "\n",
        "        message = dict(train_loss=mtrain_loss, train_mape=mtrain_mape, train_rmse=mtrain_rmse,\n",
        "                        valid_loss=mvalid_loss, valid_mape=mvalid_mape, valid_rmse=mvalid_rmse)\n",
        "        message = pd.Series(message)\n",
        "        record.append(message)\n",
        "\n",
        "        #----------------------------------------------------------------------\n",
        "        #------------------------------Check point-----------------------------\n",
        "        #----------------------------------------------------------------------\n",
        "        if check_point_exist:\n",
        "            checkpoint = torch.load(check_point_path)\n",
        "            loss_check_pt = checkpoint['loss']\n",
        "            if message.valid_loss < loss_check_pt:\n",
        "                loss_check_pt = message.valid_loss\n",
        "                torch.save({'epoch': epoch,\n",
        "                            'model_state_dict': model.state_dict(),\n",
        "                            'optimizer_state_dict': optimizer.state_dict(),\n",
        "                            'loss': message.valid_loss,\n",
        "                            }, check_point_path)\n",
        "                epochs_since_best_mae = 0\n",
        "                trigger_times = 0\n",
        "            else:\n",
        "                epochs_since_best_mae += 1\n",
        "                trigger_times += 1\n",
        "        else:\n",
        "            torch.save({'epoch': epoch,\n",
        "                        'model_state_dict': model.state_dict(),\n",
        "                        'optimizer_state_dict': optimizer.state_dict(),\n",
        "                        'loss': message.valid_loss,\n",
        "                        }, check_point_path)\n",
        "            epochs_since_best_mae = 0\n",
        "            trigger_times = 0\n",
        "            check_point_exist = True\n",
        "\n",
        "\n",
        "        #----------------------------------------------------------------------\n",
        "        #------------------------------saving record---------------------------\n",
        "        #----------------------------------------------------------------------\n",
        "        record_df = pd.DataFrame(record)\n",
        "        record_df.round(3).to_csv(record_path)\n",
        "\n",
        "        #----------------------------------------------------------------------\n",
        "        #---------------------------Displaying results-------------------------\n",
        "        #----------------------------------------------------------------------\n",
        "        log = 'Epoch: {:04d}, Training Time: {:.4f}/epoch,\\n' \\\n",
        "              'Train Loss: {:.4f}, Train MAPE: {:.4f}, Train RMSE: {:.4f}, Train MAE: {:.4f}, \\n' \\\n",
        "              'Valid Loss: {:.4f}, Valid MAPE: {:.4f}, Valid RMSE: {:.4f}, Valid MAE: {:.4f},'\n",
        "        print(log.format(epoch+1, (t2 - t1),\n",
        "                          mtrain_loss, mtrain_mape, mtrain_rmse, mtrain_mae,\n",
        "                          mvalid_loss, mvalid_mape, mvalid_rmse, mvalid_mae), flush=True)\n",
        "        print(\"*\" * 100)\n",
        "\n",
        "        #----------------------------------------------------------------------\n",
        "        #---------------------------Early stopping-----------------------------\n",
        "        #----------------------------------------------------------------------\n",
        "        if trigger_times >= patience:\n",
        "            print(\"Early stopping after {:03d} epochs!\\nStart to test process.\".format(epochs_since_best_mae))\n",
        "            break\n",
        "\n",
        "    #------------------------------------------------------------------------\n",
        "    #-------------------------Displaying time--------------------------------\n",
        "    #------------------------------------------------------------------------\n",
        "    print(\"=\" * 10)\n",
        "    print(\"Average Train Time: {:.4f} secs/epoch\".format(np.mean(train_time)))\n",
        "    print(\"Average Valid Time: {:.4f} secs\".format(np.mean(val_time)))\n",
        "    print(\"=\" * 10)\n",
        "\n",
        "\n",
        "    #------------------------------------------------------------------------\n",
        "    #------------------------------Testing-----------------------------------\n",
        "    #------------------------------------------------------------------------\n",
        "    best_epoch = np.argmin(his_loss)\n",
        "\n",
        "    checkpoint = torch.load(check_point_path)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "    epoch = checkpoint['epoch']\n",
        "    loss = checkpoint['loss']\n",
        "\n",
        "    test_mape, test_rmse, test_mae = [], [], []\n",
        "\n",
        "    for iter, (x, y) in enumerate(dataloader['test_loader'].get_iterator()):\n",
        "        testX = torch.Tensor(x).to(device).transpose(1, 2)\n",
        "        testY = torch.Tensor(y).to(device).transpose(1, 2)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            model.eval()\n",
        "            y_pred = model(testX)\n",
        "            y_pred = scaler.inverse_transform(y_pred)\n",
        "\n",
        "        evaluation = evaluate(y_pred, testY)\n",
        "        test_mape.append(evaluation[0])\n",
        "        test_rmse.append(evaluation[1])\n",
        "        test_mae.append(evaluation[2])\n",
        "\n",
        "    log = 'Evaluate on test data : Test MAPE: {:.4f}, Test RMSE: {:.4f}, Test MAE: {:.4f}'\n",
        "    print(\"=\" * 10)\n",
        "    print(\"Best epoch: \", best_epoch+1)\n",
        "    print(\"checkpoint epoch: {:03d}  loss: {:.4f}\".format(epoch, loss))\n",
        "    print(log.format(np.mean(test_mape), np.mean(test_rmse), np.mean(test_mae)))\n",
        "    print(\"=\" * 10)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EWs7ljctlMpp"
      },
      "source": [
        "# Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Madfm-nSlfKS"
      },
      "outputs": [],
      "source": [
        "#------------------------------------------------------------------------\n",
        "#------------------------Visualization Functions-------------------------\n",
        "#------------------------------------------------------------------------\n",
        "import matplotlib.dates as mdates\n",
        "\n",
        "def get_graph_path(seq_len=seq_len,\n",
        "                   pre_len=pre_len,\n",
        "                   data_name=data_name,\n",
        "                   model_name=model_name):\n",
        "    path_0 = '/content/drive/My Drive/Congestion/graph/%s/'%(model_name)\n",
        "    path_1 = '%s_seq%r_pre%r_sensor%r'%(data_name,seq_len,pre_len,sensor)\n",
        "    path = os.path.join(path_0,path_1)\n",
        "    if not os.path.exists(path_0):\n",
        "        os.makedirs(path_0)\n",
        "    return path\n",
        "\n",
        "def create_graph(y_true, y_pred, g_title, path_graph, ds_name, start_end):\n",
        "    plt.rcParams['figure.figsize']=(12, 5)\n",
        "    fig, ax = plt.subplots()\n",
        "    g_ylim_all, g_ylim_one = [0, 80], [0, 80]\n",
        "    loc='lower right'\n",
        "    g_dpi = 600 #300, 600, 800\n",
        "\n",
        "    plt.rc('xtick', labelsize=14)\n",
        "    plt.rc('ytick', labelsize=14)\n",
        "\n",
        "    a_true = y_true[start_end[0]:start_end[1]]\n",
        "    a_pred = y_pred[start_end[0]:start_end[1]]\n",
        "\n",
        "    path = '_start'+str(start_end[0])+'_end'+str(start_end[1])+'_'+g_title+'_'+str(g_dpi)+'.jpg'\n",
        "    ax.set_ylim(g_ylim_all[0], g_ylim_all[1])\n",
        "\n",
        "    ax.plot(a_true, 'b-', label='Ground Truth')\n",
        "    ax.plot(a_pred, 'r-', label='STAT')\n",
        "    ax.legend(loc=loc, ncol=2, facecolor='yellow', columnspacing=1.5, fontsize=14)\n",
        "\n",
        "    ax.set_ylabel('Traffic Speed', fontweight='bold', fontsize=14)\n",
        "    ax.set_xlabel('Time', fontweight='bold', fontsize=14)\n",
        "\n",
        "    if(g_title == 'all'):\n",
        "        ax.xaxis.set_major_formatter(mdates.DateFormatter('%m-%d'))\n",
        "\n",
        "#    for label in ax.get_xticklabels(which='major'):\n",
        "#        label.set(rotation=20, horizontalalignment='right')\n",
        "    ax.grid(True)\n",
        "    plt.savefig(path_graph+path, dpi=g_dpi)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "#------------------------------------------------------------------------\n",
        "#------------------------------Visualization-----------------------------\n",
        "#------------------------------------------------------------------------\n",
        "run = 0\n",
        "sensor = 1\n",
        "one = [2528, 2816] # sf bay(10,20,30,40)[1740, 2028]/ tf bay(100,150)[3180, 3468]/ ltp la(0,1)[2528, 2816]\n",
        "all = [2528, 5408] # sf bay(10,20,30,40)[1740, 4620]/ tf bay(100,150)[3180, 6060]/ ltp la(0,1)[2528, 5408]\n",
        "\n",
        "\n",
        "best_model_path = '_la_final0079'\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda:0\")\n",
        "    print(\"Let's use {} GPU!\".format(device))\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "\n",
        "\n",
        "trainX, trainY, valX, valY, testX, testY = get_data(seq_len, pre_len, data_name)\n",
        "d_model = trainX.shape[-1]\n",
        "\n",
        "dataloader = load_dataset(device, trainX, trainY, valX, valY, testX, testY, batch_size)\n",
        "scaler = dataloader['scaler']\n",
        "\n",
        "path_0 = '/content/drive/My Drive/Congestion/JA_model_pytorch/%s/'%(model_name)\n",
        "path_2 = '%s/epoch%r/seq%r_pre%r_run%r_'%(data_name, n_epochs, seq_len, pre_len, run+1)\n",
        "save_check_point = os.path.join(path_0,path_2)\n",
        "mkdir(save_check_point)\n",
        "\n",
        "model = STAT_model(device = device,\n",
        "                    d_model = d_model,\n",
        "                    n_layers_att = n_layers_att,\n",
        "                    n_layers = n_layers,\n",
        "                    FFN_units = FFN_units,\n",
        "                    hid = hid,\n",
        "                    dropout_rate = dropout\n",
        "                    ).to(device)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "check_point_path = save_check_point +'check_point_model'+best_model_path+'.pt'\n",
        "checkpoint = torch.load(check_point_path, map_location=torch.device('cpu'))\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "epoch = checkpoint['epoch']\n",
        "loss = checkpoint['loss']\n",
        "\n",
        "test_mape, test_rmse, test_mae = [], [], []\n",
        "\n",
        "testX = torch.Tensor(testX).to(device).transpose(1, 2)\n",
        "testY = torch.Tensor(testY).to(device).transpose(1, 2)\n",
        "\n",
        "for iter, (x, y) in enumerate(dataloader['test_loader'].get_iterator()):\n",
        "    testX = torch.Tensor(x).to(device).transpose(1, 2)\n",
        "    testY = torch.Tensor(y).to(device).transpose(1, 2)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        model.eval()\n",
        "        predY = model(testX)\n",
        "        predY = scaler.inverse_transform(predY)\n",
        "    if (iter == 0):\n",
        "        testY_1 = testY\n",
        "        predY_1 = predY\n",
        "    else:\n",
        "        testY_1 = torch.cat((testY_1, testY), 0)\n",
        "        predY_1 = torch.cat((predY_1, predY), 0)\n",
        "\n",
        "y_true = testY_1[:, sensor, -1].to(\"cpu\")\n",
        "y_pred = predY_1[:, sensor, -1].to(\"cpu\")\n",
        "y_true = pd.DataFrame(y_true)\n",
        "y_pred = pd.DataFrame(y_pred)\n",
        "time_len, d_model = y_true.shape\n",
        "rng = pd.date_range(end=test_date, periods=time_len, freq=freq)\n",
        "y_true.index = rng\n",
        "y_pred.index = rng\n",
        "path_graph = get_graph_path(seq_len, pre_len, data_name, model_name)\n",
        "\n",
        "create_graph(y_true, y_pred, 'one', path_graph, data_name, one)\n",
        "create_graph(y_true, y_pred, 'all', path_graph, data_name, all)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}